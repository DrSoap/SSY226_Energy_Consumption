{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generator main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras                  import regularizers\n",
    "from keras                  import Input, Model, Sequential\n",
    "from keras.layers           import Bidirectional, Activation, TimeDistributed, Dense, RepeatVector, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.utils            import np_utils\n",
    "from keras.callbacks        import EarlyStopping, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "temp_path = data_path + '/temp'\n",
    "backup_path = data_path + '/backup'\n",
    "weights_path = data_path + '/weights'\n",
    "train_history = data_path + '/train_history'\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "if not os.path.isdir(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "if not os.path.isdir(backup_path):\n",
    "    os.mkdir(backup_path)\n",
    "if not os.path.isdir(weights_path):\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.isdir(train_history):\n",
    "    os.mkdir(train_history)\n",
    "\n",
    "with open(data_path + '/joint_angle_data.pickle', 'rb') as file:\n",
    "    joint_angle_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open(data_path + '/power_data.pickle', 'rb') as file:\n",
    "    power_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToLSTM(self, multiple=True):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[1]\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(LSTM(64, return_sequences = multiple, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    self.model.add(Dense(16, activation='relu'))\n",
    "    self.model.add(Dense(n_outputs))\n",
    "    self.model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(dataX, dataY, n_input, n_output):\n",
    "    #Output: [batchs, n_timesteps, n_features]\n",
    "    df_X = pd.DataFrame(dataX, columns=list('ABCDEF'))\n",
    "    df_Y = pd.DataFrame(dataY, columns=list('A'))\n",
    "    X = np.ndarray((int(df_X.shape[0]/n_input), n_input, df_X.shape[1]) )\n",
    "    Y = np.ndarray((int(df_Y.shape[0]/n_input), n_output))\n",
    "\n",
    "    for n in range(1,df_X.shape[0]+1,n_input):\n",
    "        for m in range(n_input-1,-1, -1):\n",
    "            X[int((n-1)/n_input),m,:] = df_X[n+m-1:n+m]\n",
    "            \n",
    "    for n in range(1,df_Y.shape[0]+1,n_output):\n",
    "        for m in range(n_input-1,-1, -1):\n",
    "            Y[int((n-1)/n_output),:] = df_Y[n+m-1:n+m]\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpData(self, seq_length, dataX, dataY, n_test_ratio):\n",
    "\n",
    "    self.X_train = []\n",
    "    self.Y_train = []\n",
    "    self.X_test = []\n",
    "    self.Y_test = []\n",
    "    self.seq_length = seq_length\n",
    "    \n",
    "    X, Y = series_to_supervised(dataX, dataY, n_input=seq_length, n_output=dataY.shape[1])\n",
    "    \n",
    "    self.X_train = X[:int(len(X)*(1-n_test_ratio)), :, :]\n",
    "    self.Y_train = Y[:int(len(Y)*(1-n_test_ratio)), :]\n",
    "    self.X_test = X[int(len(X)*(1-n_test_ratio)):, :, :]\n",
    "    self.Y_test = Y[int(len(Y)*(1-n_test_ratio)):, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(self, name):\n",
    "    # File path for model\n",
    "    filepath = weights_path + \"/weights-\" + name + \"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    # Callbacks functions\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32)\n",
    "    mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    \n",
    "    # Train the model\n",
    "    epochs = 10           #Maximum number of epochs to run\n",
    "    batch_size = 1    #Size of training data batch\n",
    "    Val_split = 0.1       #Procentage of training data to use as validation data\n",
    "    history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size, validation_split=Val_split, callbacks=[es, tb, mc])\n",
    "    \n",
    "    # Save the model\n",
    "    filename = \"model_\" + name + \".hdf5\"\n",
    "    self.model.save_weights(weights_path + '/' + filename)\n",
    "    # Save the history\n",
    "    filename = \"history_\" + name + \".pickle\"\n",
    "    with open(train_history + '/' + filename, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    set_up_data = setUpData\n",
    "    train_model = trainModel\n",
    "    \n",
    "    set_model_to_RNN = setModelToRNN\n",
    "    set_model_to_GRU = setModelToGRU\n",
    "    set_model_to_LSTM = setModelToLSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 1 #12ms/leng unit\n",
    "batch_size = 8\n",
    "n_test_ratio = 0.02\n",
    "print((power_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_LSTM(multiple=False)\n",
    "name = 'test_run'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
