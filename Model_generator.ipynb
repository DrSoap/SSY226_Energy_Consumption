{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generator main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection     import GridSearchCV\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints           import maxnorm\n",
    "from keras                       import regularizers\n",
    "from keras                       import Input, Model, Sequential\n",
    "from keras.layers                import Flatten, TimeDistributed, CuDNNGRU, CuDNNLSTM, Bidirectional, Activation, TimeDistributed, Dense, RepeatVector, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers.recurrent      import LSTM, GRU, SimpleRNN\n",
    "from keras.utils                 import np_utils\n",
    "from keras.callbacks             import EarlyStopping, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "temp_path = data_path + '/temp'\n",
    "backup_path = data_path + '/backup'\n",
    "weights_path = data_path + '/weights'\n",
    "train_history = data_path + '/train_history'\n",
    "resource_path = 'Pickled_data'\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "if not os.path.isdir(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "if not os.path.isdir(backup_path):\n",
    "    os.mkdir(backup_path)\n",
    "if not os.path.isdir(weights_path):\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.isdir(train_history):\n",
    "    os.mkdir(train_history)\n",
    "\n",
    "#joint_angle_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_auginput.pickle', 'rb') as file:\n",
    "    joint_angle_data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "#power_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_poweroutput.pickle', 'rb') as file:\n",
    "    power_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "print('===== Check sizes =====')\n",
    "print('joint_angle_data shape is: ')\n",
    "print(joint_angle_data.shape)\n",
    "print('power_data shape is: ')\n",
    "print(power_data.shape)\n",
    "print('First value needs to be equal!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToBasicLSTM(self, multiple=False):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = multiple, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    if multiple:\n",
    "        self.model.add(TimeDistributed(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        self.model.add(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(n_outputs))\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=[mean_square_error])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToGridSearch(self, multiple=False, layer_type=CuDNNLSTM, hidden_units_RNN=128, hidden_units_dense=64, num_mid_layers = 3, optimizer='adam', init_mode='random_uniform', activation='relu', dropout_rate=0.2, num_dense_layers=1):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = True, kernel_initializer = init_mode, bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    for n in range(num_mid_layers):\n",
    "        self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = True, kernel_initializer = init_mode, bias_initializer = 'zero')))\n",
    "        self.model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = multiple, kernel_initializer = init_mode, bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if multiple:\n",
    "        for n in range(num_dense_layers):\n",
    "            self.model.add(TimeDistributed(Dense(hidden_units_dense, activation=activation, kernel_initializer = 'random_uniform', bias_initializer = 'zero'))) #, kernel_constraint=maxnorm(weight_constraint)\n",
    "    \n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        for n in range(num_dense_layers):\n",
    "            self.model.add(Dense(hidden_units_dense, activation=activation, kernel_initializer = 'random_uniform', bias_initializer = 'zero')) #, kernel_constraint=maxnorm(weight_constraint)\n",
    "    \n",
    "        self.model.add(Dense(n_outputs))\n",
    "        \n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToFFNN(self):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[1]\n",
    "\n",
    "    inputs = Input(shape=(n_timesteps,n_features))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    prediction = Dense(n_outputs, activation='relu')(x)\n",
    "    \n",
    "    self.model = Model(inputs, prediction)\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=[mean_square_error])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToFinalModel(self, multiple=False):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128, return_sequences = multiple, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    if multiple:\n",
    "        self.model.add(TimeDistributed(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        self.model.add(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(n_outputs))\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=[mean_square_error])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(dataX, dataY, n_input, n_output, multiple=False):\n",
    "    #Output: [batchs, n_timesteps, n_features]\n",
    "    df_X = pd.DataFrame(dataX)\n",
    "    df_Y = pd.DataFrame(dataY)\n",
    "    X = np.ndarray((df_X.shape[0]-n_input, n_input, df_X.shape[1]) )\n",
    "    if multiple:\n",
    "        Y = np.ndarray((df_Y.shape[0]-n_input, n_input, n_output))\n",
    "    else:\n",
    "        Y = np.ndarray((df_Y.shape[0]-n_input, n_output))\n",
    "\n",
    "    for n in range(0,df_X.shape[0]-n_input-1,1):\n",
    "        for m in range(n_input-1,-1, -1):\n",
    "            X[n,m,:] = df_X.loc[n+m:n+m,:]\n",
    "    if multiple:\n",
    "        for n in range(0,df_Y.shape[0]-n_input-1,1):\n",
    "            for m in range(n_input-1,-1, -1):\n",
    "                Y[n,m,:] = df_Y.loc[n+m:n+m,:]\n",
    "    else:\n",
    "        for n in range(0,df_Y.shape[0]-n_input-1,1):\n",
    "            for m in range(n_output-1,-1, -1):\n",
    "                Y[n,:] = df_Y.loc[n+m:n+m,:]\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpData(self, seq_length, dataX, dataY, n_test_ratio, multiple=False):\n",
    "\n",
    "    self.X_train = []\n",
    "    self.Y_train = []\n",
    "    self.X_test = []\n",
    "    self.Y_test = []\n",
    "    self.seq_length = seq_length\n",
    "    \n",
    "    X, Y = series_to_supervised(dataX, dataY, multiple=multiple, n_input=seq_length, n_output=dataY.shape[1])\n",
    "    \n",
    "    self.X_train = X[:int(len(X)*(1-n_test_ratio)), :, :]\n",
    "    self.Y_train = Y[:int(len(Y)*(1-n_test_ratio)), :]\n",
    "    self.X_test = X[int(len(X)*(1-n_test_ratio)):, :, :]\n",
    "    self.Y_test = Y[int(len(Y)*(1-n_test_ratio)):, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(self, name):\n",
    "    # File path for model\n",
    "    filepath = weights_path + \"/weights-\" + name + \"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    # Callbacks functions\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32)\n",
    "    mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    \n",
    "    # Train the model\n",
    "    epochs = 100           #Maximum number of epochs to run\n",
    "    batch_size = 32    #Size of training data batch\n",
    "    Val_split = 0.1       #Procentage of training data to use as validation data\n",
    "    history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size, validation_split=Val_split, callbacks=[es, tb, mc])\n",
    "    \n",
    "    # Save the model\n",
    "    filename = \"model_\" + name + \".hdf5\"\n",
    "    self.model.save_weights(weights_path + '/' + filename)\n",
    "    # Save the history\n",
    "    filename = \"history_\" + name + \".pickle\"\n",
    "    with open(train_history + '/' + filename, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    set_up_data = setUpData\n",
    "    train_model = trainModel\n",
    "    \n",
    "    set_model_to_Basic_LSTM = setModelToBasicLSTM\n",
    "    set_model_to_Grid_Search = setModelToGridSearch\n",
    "    set_model_to_FFNN = setModelToFFNN\n",
    "    set_model_to_final_model = setModelToFinalModel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 40 #12ms/leng unit\n",
    "n_test_ratio = 0.02\n",
    "print((power_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio, multiple=False)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_FFNN()\n",
    "name = 'test_run_FFNN'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 40 #12ms/leng unit\n",
    "n_test_ratio = 0.02\n",
    "multiple = True\n",
    "print((power_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_Basic_LSTM(multiple=multiple)\n",
    "name = 'test_run_Basic_LSTM'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_gs = Generator()\n",
    "# Variables\n",
    "seqLength = 40 #12ms/leng unit\n",
    "n_test_ratio = 0\n",
    "multiple = False\n",
    "# Set up the data\n",
    "print('Setting up data')\n",
    "gen_gs.set_up_data(seqLength, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "# create model\n",
    "print('Creating keras Classifier')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model = KerasClassifier(build_fn=gen_gs.set_model_to_Grid_Search, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "multiple = [multiple]\n",
    "layer_type = [CuDNNLSTM, CuDNNGRU]\n",
    "hidden_units_RNN = [64, 128]\n",
    "hidden_units_dense = [64, 128]\n",
    "num_dense_layers = [1, 2]\n",
    "num_mid_layers = [0, 2, 4]\n",
    "#optimizer = ['adam']\n",
    "#learn_rate = [0.01]\n",
    "#momentum = [0]\n",
    "#init_mode = ['random_uniform']\n",
    "#activation = ['relu']\n",
    "dropout_rate = [0, 0.2]\n",
    "#weight_constraint = [0]\n",
    "# Make dictionary\n",
    "param_grid = dict(multiple=multiple,\n",
    "                  layer_type=layer_type,\n",
    "                  hidden_units_dense=hidden_units_dense,\n",
    "                  hidden_units_RNN=hidden_units_RNN,\n",
    "                  num_mid_layers=num_mid_layers,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  num_dense_layers=num_dense_layers)\n",
    "# Grid Search\n",
    "print('Grid Search Starting')\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=100)\n",
    "print('X: ' + str(gen_gs.X_train.shape))\n",
    "print('Y: ' + str(gen_gs.Y_train.shape))\n",
    "grid_result = grid.fit(gen_gs.X_train, gen_gs.Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results from the Grid Search\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [['mean_test_score', 'std_test_score', 'params']]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    mylist.append([mean, stdev, param])\n",
    "    \n",
    "with open('Grid_Search_Result.csv', 'w') as myfile:\n",
    "    #wr = csv.writer(myfile, dialect='excel')\n",
    "    for row in mylist:\n",
    "        for column in row:\n",
    "            myfile.write('%s;' % column)\n",
    "        myfile.write('\\n')\n",
    "    myfile.close()\n",
    "print('Done Saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gen.model.predict(gen.X_train)\n",
    "predictions_test = gen.model.predict(gen.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_interval = [100:1000]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig1 = plt.subplot()\n",
    "x1 = range(len(predictions_train))\n",
    "fig1.plot(x1[time_interval], gen.Y_train[time_interval],  label='true',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig1.plot(x1[time_interval], predictions_train[time_interval],  label='pred',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig1.legend();\n",
    "fig1.set_ylabel('Power[W]')\n",
    "fig1.set_xlabel('Time')\n",
    "fig1.set_title('Train data prediction');\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig2 = plt.subplot()\n",
    "x2 = range(len(predictions_test))\n",
    "fig2.plot(x2, gen.Y_test,  label='true',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig2.plot(x2, predictions_test,  label='pred',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig2.legend();\n",
    "fig2.set_ylabel('Power[W]')\n",
    "fig2.set_xlabel('Time')\n",
    "fig2.set_title('Test data prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for final training\n",
    "path_to_data_mapp = 'Pickled_data'\n",
    "gen_final = Generator()\n",
    "seqLleng = 25 #12ms/leng unit\n",
    "n_test_ratio = 0\n",
    "num_of_test_paths = 1\n",
    "gen_final.set_model_to_final_model()\n",
    "\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(folder_path):\n",
    "    path = os.path.join(folder_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(name)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(name)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training\n",
    "for n in range(len(joint_angle_data_path_list)-num_of_test_paths):\n",
    "    #joint_angle_data.pickle\n",
    "    with open(path_to_data_mapp + joint_angle_data_path_list(n), 'rb') as file:\n",
    "        joint_angle_data = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #power_data.pickle\n",
    "    with open(path_to_data_mapp + power_data_path_list(match_list_vector(n)), 'rb') as file:\n",
    "        power_data = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    gen_final.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio)\n",
    "    name = 'Final_Model_Iteration_' + str(n)\n",
    "    gen_final.train_model(name = name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Resulting figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = gen.model.predict(gen.X_test)\n",
    "time_interval = [100:1000]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig = plt.subplot()\n",
    "x = range(len(predictions_test))\n",
    "fig.plot(x[time_interval], gen.Y_test[time_interval],  label='true',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig.plot(x[time_interval], predictions_test[time_interval],  label='pred',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig.legend();\n",
    "fig.set_ylabel('Power [W]')\n",
    "fig.set_xlabel('Time [s]')\n",
    "fig.set_title('Test data prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
