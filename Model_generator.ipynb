{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generator main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras                  import regularizers\n",
    "from keras                  import Input, Model, Sequential\n",
    "from keras.layers           import Bidirectional, Activation, TimeDistributed, Dense, RepeatVector, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.utils            import np_utils\n",
    "from keras.callbacks        import EarlyStopping, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "temp_path = data_path + '/temp'\n",
    "backup_path = data_path + '/backup'\n",
    "weights_path = data_path + '/weights'\n",
    "train_history = data_path + '/train_history'\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "if not os.path.isdir(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "if not os.path.isdir(backup_path):\n",
    "    os.mkdir(backup_path)\n",
    "if not os.path.isdir(weights_path):\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.isdir(train_history):\n",
    "    os.mkdir(train_history)\n",
    "\n",
    "with open(data_path + '/joint_angle_data.pickle', 'rb') as file:\n",
    "    joint_angle_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open(data_path + '/power_data.pickle', 'rb') as file:\n",
    "    power_data = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToRNN(self):\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(SimpleRNN(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (self.X.shape[1], self.X.shape[2]))))\n",
    "    #self.model.add(BatchNormalization())\n",
    "    #self.model.add(Bidirectional(SimpleRNN(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', go_backwards = False)))\n",
    "    #self.model.add(BatchNormalization())\n",
    "    #self.model.add(Bidirectional(SimpleRNN(64, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Dense(self.y.shape[1], activation='softmax'))\n",
    "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToGRU(self):\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(GRU(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (self.X.shape[1], self.X.shape[2]))))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Bidirectional(GRU(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', go_backwards = False)))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Bidirectional(GRU(64, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Dense(self.y.shape[1], activation='softmax'))\n",
    "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToLSTM(self):\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(LSTM(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (self.X.shape[1], self.X.shape[2]))))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Bidirectional(LSTM(64, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Bidirectional(LSTM(64, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(BatchNormalization())\n",
    "    self.model.add(Dense(self.y.shape[1], activation='softmax'))\n",
    "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpData(self, seq_length):\n",
    "\n",
    "    self.data_history = []\n",
    "    self.data_label = []\n",
    "    self.seq_length = seq_length\n",
    "    \n",
    "    for n in range(0,int(len(joint_angle_data)/seq_length)):\n",
    "        self.data_history.append(np.array(joint_angle_data[n*seq_length:(n+1)*seq_length]).T)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_history = []\n",
    "data_label = []\n",
    "seq_length = 10\n",
    "    \n",
    "for n in range(0,int(len(joint_angle_data)/seq_length)):\n",
    "    data_history.append(np.array(joint_angle_data[n*seq_length:(n+1)*seq_length][1:len(joint_angle_data[0])]).T)\n",
    "    #data_label.append()\n",
    "\n",
    "#for n, TI_data in enumerate(joint_angle_data):\n",
    "#        print(n)\n",
    "#len(joint_angle_data)\n",
    "\n",
    "print(len(data_history))\n",
    "print(np.array(joint_angle_data[0:seq_length][1:len(joint_angle_data[0])+1]).T)\n",
    "\n",
    "print(joint_angle_data[0:seq_length])\n",
    "\n",
    "print(joint_angle_data[0])\n",
    "print(joint_angle_data[0][1:len(joint_angle_data[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(self, name):\n",
    "    # File path for model\n",
    "    filepath = weights_path + \"/weights-\" + name + \"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    # Callbacks functions\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32)\n",
    "    mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    # Train the model\n",
    "    epochs = 20           #Maximum number of epochs to run\n",
    "    batch_size = 128*2    #Size of training data batch\n",
    "    Val_split = 0.1       #Procentage of training data to use as validation data\n",
    "    history = self.model.fit(self.X, self.y, epochs=epochs, batch_size=batch_size, validation_split=Val_split, callbacks=[es, tb, mc])\n",
    "    # Save the model\n",
    "    filename = \"model_\" + name + \".hdf5\"\n",
    "    self.model.save_weights(weights_path + '/' + filename)\n",
    "    # Save the history\n",
    "    filename = \"history_\" + name + \".pickle\"\n",
    "    with open(train_history + '/' + filename, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    set_up_data = setUpData\n",
    "    train_model = trainModel\n",
    "    \n",
    "    set_model_to_RNN = setModelToRNN\n",
    "    set_model_to_GRU = setModelToGRU\n",
    "    set_model_to_LSTM = setModelToLSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "leng = 64\n",
    "gen.set_up_data(leng)\n",
    "gen.set_model_to_LSTM()\n",
    "name = 'test_run'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
