{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generator main file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection     import GridSearchCV\n",
    "from sklearn.metrics             import average_precision_score\n",
    "from sklearn.model_selection     import KFold, cross_val_score\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints           import maxnorm\n",
    "from keras                       import regularizers\n",
    "from keras                       import Input, Model, Sequential\n",
    "from keras.layers                import Flatten, TimeDistributed, CuDNNGRU, CuDNNLSTM, Bidirectional, Activation, TimeDistributed, Dense, RepeatVector, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers.recurrent      import LSTM, GRU, SimpleRNN\n",
    "from keras.utils                 import np_utils\n",
    "from keras.callbacks             import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.optimizers            import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and make folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "temp_path = data_path + '/temp'\n",
    "backup_path = data_path + '/backup'\n",
    "weights_path = data_path + '/weights'\n",
    "train_history = data_path + '/train_history'\n",
    "resource_path = 'Pickled_data'\n",
    "test_set_path = resource_path + '/test_sets'\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "if not os.path.isdir(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "if not os.path.isdir(backup_path):\n",
    "    os.mkdir(backup_path)\n",
    "if not os.path.isdir(weights_path):\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.isdir(train_history):\n",
    "    os.mkdir(train_history)\n",
    "if not os.path.isdir(test_set_path):\n",
    "    os.mkdir(train_history)\n",
    "\n",
    "#joint_angle_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_auginput.pickle', 'rb') as file:\n",
    "    joint_angle_data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "#power_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_poweroutput.pickle', 'rb') as file:\n",
    "    power_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "#joint_angle_data = joint_angle_data[:,:-6]\n",
    "print('===== Check sizes =====')\n",
    "print('joint_angle_data shape is: ')\n",
    "print(joint_angle_data.shape)\n",
    "print('power_data shape is: ')\n",
    "print(power_data.shape)\n",
    "print('First value needs to be equal!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToBasicLSTM(self, multiple=False):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[1]\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(128*2, return_sequences = multiple, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    if multiple:\n",
    "        self.model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        self.model.add(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(128, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(n_outputs))\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer=rmsprop, metrics=[mean_square_error])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToGridSearch(self, multiple=False, layer_type=CuDNNLSTM, hidden_units_RNN=128, hidden_units_dense=64, num_mid_layers = 3, optimizer='adam', init_mode='random_uniform', activation='relu', dropout_rate=0.2, num_dense_layers=1):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = True, kernel_initializer = init_mode, bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    for n in range(num_mid_layers):\n",
    "        self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = True, kernel_initializer = init_mode, bias_initializer = 'zero')))\n",
    "        self.model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    self.model.add(Bidirectional(layer_type(hidden_units_RNN, return_sequences = multiple, kernel_initializer = init_mode, bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if multiple:\n",
    "        for n in range(num_dense_layers):\n",
    "            self.model.add(TimeDistributed(Dense(hidden_units_dense, activation=activation, kernel_initializer = 'random_uniform', bias_initializer = 'zero'))) #, kernel_constraint=maxnorm(weight_constraint)\n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        for n in range(num_dense_layers):\n",
    "            self.model.add(Dense(hidden_units_dense, activation=activation, kernel_initializer = 'random_uniform', bias_initializer = 'zero')) #, kernel_constraint=maxnorm(weight_constraint)\n",
    "        self.model.add(Dense(n_outputs))\n",
    "        \n",
    "    self.model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=[\"accuracy\"])   \n",
    "    return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToFFNN(self):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # define model\n",
    "    inputs = Input(shape=(n_timesteps,n_features))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    prediction = Dense(n_outputs, activation='relu')(x)\n",
    "    \n",
    "    self.model = Model(inputs, prediction)\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer=rmsprop, metrics=[mean_square_error])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModelToFinalModel(self, multiple=False):\n",
    "    n_timesteps, n_features, n_outputs = self.X_train.shape[1], self.X_train.shape[2], self.Y_train.shape[-1]\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    rmsprop = RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # define model\n",
    "    self.model = Sequential()\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero', input_shape = (n_timesteps, n_features))))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = True, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    self.model.add(Bidirectional(CuDNNLSTM(256, return_sequences = multiple, kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "    self.model.add(Dropout(0.2))\n",
    "    if multiple:\n",
    "        self.model.add(TimeDistributed(Dense(64, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "        self.model.add(TimeDistributed(Dense(64, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero')))\n",
    "        self.model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    else:\n",
    "        self.model.add(Dense(64, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(64, activation='relu', kernel_initializer = 'random_uniform', bias_initializer = 'zero'))\n",
    "        self.model.add(Dense(n_outputs))\n",
    "    self.model.compile(loss='mean_absolute_percentage_error', optimizer=rmsprop, metrics=['mean_absolute_percentage_error',mean_square_error])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(dataX, dataY, n_input, n_output, multiple=False):\n",
    "    #Output: [batchs, n_timesteps, n_features]\n",
    "    df_X = pd.DataFrame(dataX)\n",
    "    df_Y = pd.DataFrame(dataY)\n",
    "    X = np.ndarray((df_X.shape[0]-n_input, n_input, df_X.shape[1]) )\n",
    "    if multiple:\n",
    "        Y = np.ndarray((df_Y.shape[0]-n_input, n_input, n_output))\n",
    "    else:\n",
    "        Y = np.ndarray((df_Y.shape[0]-n_input, n_output))\n",
    "\n",
    "    for n in range(0,df_X.shape[0]-n_input-1,1):\n",
    "        for m in range(n_input-1,-1, -1):\n",
    "            X[n,m,:] = df_X.loc[n+m:n+m,:]\n",
    "            \n",
    "    if multiple:\n",
    "        for n in range(0,df_Y.shape[0]-n_input-1,1):\n",
    "            for m in range(n_input-1,-1, -1):\n",
    "                Y[n,m,:] = df_Y.loc[n+m:n+m,:]\n",
    "    else:\n",
    "        for n in range(0,df_Y.shape[0]-n_input-1,1):\n",
    "            for m in range(n_output-1,-1, -1):\n",
    "                Y[n,:] = df_Y.loc[n+m:n+m,:]\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpData(self, seq_length, dataX, dataY, n_test_ratio, multiple=False):\n",
    "    self.X_train = []\n",
    "    self.Y_train = []\n",
    "    self.X_test = []\n",
    "    self.Y_test = []\n",
    "    self.seq_length = seq_length\n",
    "    \n",
    "    X, Y = series_to_supervised(dataX, dataY, multiple=multiple, n_input=seq_length, n_output=dataY.shape[1])\n",
    "    \n",
    "    self.X_train = X[:int(len(X)*(1-n_test_ratio)), :, :]\n",
    "    self.Y_train = Y[:int(len(Y)*(1-n_test_ratio)), :]\n",
    "    self.X_test = X[int(len(X)*(1-n_test_ratio)):, :, :]\n",
    "    self.Y_test = Y[int(len(Y)*(1-n_test_ratio)):, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpMultiData(self, seq_length, dataX_paths, dataY_paths, match_list_vector, num_loaded, limited_data=False, multiple=False):\n",
    "    \n",
    "    self.seq_length = seq_length\n",
    "    num_loaded_loop = 0\n",
    "    for n in range(len(dataX_paths)):\n",
    "        print('Work: ' + str(n+1) + ' of ' + str(len(dataX_paths)))\n",
    "        #joint_angle_data.pickle\n",
    "        with open(dataX_paths[n], 'rb') as file:\n",
    "            joint_angle_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        #power_data.pickle\n",
    "        with open(dataY_paths[match_list_vector[n]], 'rb') as file:\n",
    "            power_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        if(int(joint_angle_data.shape[0]) != int(power_data.shape[0])):\n",
    "            print('Files not equal in sizes! could not use.')\n",
    "            continue\n",
    "\n",
    "        X, Y = series_to_supervised(joint_angle_data, power_data, multiple=multiple, \n",
    "                                    n_input=seq_length, n_output=power_data.shape[-1])\n",
    "        \n",
    "        num_loaded_loop = num_loaded_loop + 1\n",
    "        if( num_loaded_loop == 1):\n",
    "            self.X_train = X\n",
    "            self.Y_train = Y\n",
    "            print(self.X_train.shape)\n",
    "            print(self.Y_train.shape)\n",
    "            continue\n",
    "    \n",
    "        randomNum = random.sample(range(0, len(X)), 6000)\n",
    "        if multiple:\n",
    "            #for num in range(5000):\n",
    "            self.X_train = np.concatenate((self.X_train,X[randomNum,:,:]))\n",
    "            self.Y_train = np.concatenate((self.Y_train,Y[randomNum,:,:]))\n",
    "            print(self.X_train.shape)\n",
    "            print(self.Y_train.shape)\n",
    "        else:\n",
    "            #for num in range(5000):\n",
    "            self.X_train = np.concatenate((self.X_train,X[randomNum,:,:]))\n",
    "            self.Y_train = np.concatenate((self.Y_train,Y[randomNum,:]))\n",
    "            print(self.X_train.shape)\n",
    "            print(self.Y_train.shape)\n",
    "\n",
    "        if(num_loaded_loop>num_loaded):\n",
    "            print('\"num_loaded\" reached stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(self, name):\n",
    "    # File path for model\n",
    "    filepath = weights_path + \"/weights-\" + name + \"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    # Callbacks functions\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32)\n",
    "    mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    \n",
    "    # Train the model\n",
    "    epochs = 500           #Maximum number of epochs to run\n",
    "    batch_size = 128    #Size of training data batch\n",
    "    Val_split = 0.1       #Procentage of training data to use as validation data\n",
    "    history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size, validation_split=Val_split, callbacks=[es, tb, mc])\n",
    "    \n",
    "    # Save the model\n",
    "    filename = \"model_\" + name + \".hdf5\"\n",
    "    self.model.save_weights(weights_path + '/' + filename)\n",
    "    # Save the history\n",
    "    filename = \"history_\" + name + \".pickle\"\n",
    "    with open(train_history + '/' + filename, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPreProcessedData(self, Data_X_name, Data_Y_name):\n",
    "    #Data_X_name\n",
    "    with open(Data_X_name, 'rb') as file:\n",
    "        self.X_train = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #Data_Y_name\n",
    "    with open(Data_Y_name, 'rb') as file:\n",
    "        self.Y_train = pickle.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTrain(self):\n",
    "    epochs = 1           #Maximum number of epochs to run\n",
    "    batch_size = 128     #Size of training data batch\n",
    "    Val_split = 0.1       #Procentage of training data to use as validation data\n",
    "    history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size, validation_split=Val_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    set_up_data = setUpData\n",
    "    set_up_multi_data = setUpMultiData\n",
    "    train_model = trainModel\n",
    "    load_pre_processed_data = loadPreProcessedData\n",
    "    pre_train = preTrain\n",
    "    \n",
    "    set_model_to_Basic_LSTM = setModelToBasicLSTM\n",
    "    set_model_to_Grid_Search = setModelToGridSearch\n",
    "    set_model_to_FFNN = setModelToFFNN\n",
    "    set_model_to_final_model = setModelToFinalModel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 25 #12ms/leng unit\n",
    "n_test_ratio = 0.02\n",
    "print((power_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio, multiple=False)\n",
    "print(type(gen.X_train))\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_FFNN()\n",
    "name = 'test_run_FFNN'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training old data Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_data = False\n",
    "if old_data:\n",
    "    #joint_angle_data.pickle\n",
    "    with open('data/augmented_input.pickle', 'rb') as file:\n",
    "        joint_angle_data = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #power_data.pickle\n",
    "    with open('data/power_data.pickle', 'rb') as file:\n",
    "        power_data = pickle.load(file)\n",
    "        file.close()\n",
    "else:\n",
    "    #joint_angle_data.pickle\n",
    "    with open(resource_path + '/100_percent_path4_high_auginput.pickle', 'rb') as file:\n",
    "        joint_angle_data = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #power_data.pickle\n",
    "    with open(resource_path + '/100_percent_path4_high_poweroutput.pickle', 'rb') as file:\n",
    "        power_data = pickle.load(file)\n",
    "        file.close()\n",
    "    #joint_angle_data = joint_angle_data[:,:-6]\n",
    "\n",
    "gen = Generator()\n",
    "seqLleng = 100 #12ms/leng unit\n",
    "n_test_ratio = 0.02\n",
    "multiple = False\n",
    "print((power_data.shape))\n",
    "print((joint_angle_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio, multiple=False)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_Basic_LSTM(multiple=multiple)\n",
    "name = 'test_run_Basic_LSTM'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training current loaded data - basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 100 #12ms/leng unit\n",
    "n_test_ratio = 0.00\n",
    "multiple = False\n",
    "print((joint_angle_data.shape))\n",
    "print((power_data.shape))\n",
    "gen.set_up_data(seqLleng, joint_angle_data, power_data, n_test_ratio, multiple=False)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_Basic_LSTM(multiple=multiple)\n",
    "name = 'test_run_Basic_LSTM'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training all files together (can be modified to load just some certain types) basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "seqLleng = 100 #12ms/leng unit\n",
    "n_test_ratio = 0.02\n",
    "multiple = False\n",
    "#print((power_data.shape))\n",
    "\n",
    "folder_path = 'Pickled_data/'\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(folder_path):\n",
    "    path = os.path.join(folder_path, name)\n",
    "    if \"_high_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_high_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_high_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "        \n",
    "print(joint_angle_data_path_list[1])\n",
    "print(power_data_path_list[match_list_vector[1]])\n",
    "num_loaded = 3\n",
    "n_test = 0\n",
    "gen.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, n_test, num_loaded, multiple=multiple)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "print(gen.X_test.shape)\n",
    "print(gen.Y_test.shape)\n",
    "gen.set_model_to_Basic_LSTM(multiple=multiple)\n",
    "name = 'test_run_Basic_LSTM'\n",
    "gen.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_gs = Generator()\n",
    "# Variables\n",
    "seqLength = 100 #12ms/leng unit\n",
    "n_test_ratio = 0\n",
    "multiple = False\n",
    "# Set up the data\n",
    "print('Setting up data')\n",
    "gen_gs.set_up_data(seqLength, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "# create model\n",
    "print('Creating keras Classifier')\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "model = KerasClassifier(build_fn=gen_gs.set_model_to_Grid_Search, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "multiple = [multiple]\n",
    "layer_type = [CuDNNLSTM]; #[CuDNNLSTM, CuDNNGRU]\n",
    "hidden_units_RNN = [128, 256]\n",
    "hidden_units_dense = [64, 128]\n",
    "num_dense_layers = [1, 2]\n",
    "num_mid_layers = [0, 2, 4]\n",
    "#optimizer = ['adam']\n",
    "#learn_rate = [0.01]\n",
    "#momentum = [0]\n",
    "#init_mode = ['random_uniform']\n",
    "#activation = ['relu']\n",
    "dropout_rate = [0.2]\n",
    "#weight_constraint = [0]\n",
    "# Make dictionary\n",
    "param_grid = dict(layer_type =layer_type,\n",
    "                  hidden_units_dense=hidden_units_dense,\n",
    "                  hidden_units_RNN=hidden_units_RNN,\n",
    "                  num_mid_layers=num_mid_layers,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  num_dense_layers=num_dense_layers)\n",
    "# Grid Search\n",
    "print('Grid Search Starting')\n",
    "k_fold = KFold(n_splits=2)\n",
    "grid = GridSearchCV(estimator=model, cv=k_fold, scoring='mean_squared_error', param_grid=param_grid, n_jobs=1, verbose=100)\n",
    "print('X: ' + str(gen_gs.X_train.shape))\n",
    "print('Y: ' + str(gen_gs.Y_train.shape))\n",
    "grid_result = grid.fit(gen_gs.X_train, gen_gs.Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from Grid Search, which summarizes the data and saves the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results from the Grid Search\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results from the Grid Search\n",
    "mylist = [['mean_test_score', 'std_test_score', 'params']]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    mylist.append([mean, stdev, param])\n",
    "    \n",
    "with open('Grid_Search_Result.csv', 'w') as myfile:\n",
    "    #wr = csv.writer(myfile, dialect='excel')\n",
    "    for row in mylist:\n",
    "        for column in row:\n",
    "            myfile.write('%s;' % column)\n",
    "        myfile.write('\\n')\n",
    "    myfile.close()\n",
    "print('Done Saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using train and test data and plot the result (OLD DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gen.model.predict(gen.X_train)\n",
    "predictions_test = gen.model.predict(gen.X_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig1 = plt.subplot()\n",
    "x1 = range(len(predictions_train))\n",
    "fig1.plot(x1[100:1000], gen.Y_train[100:1000],  label='true',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig1.plot(x1[100:1000], predictions_train[100:1000],  label='pred',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig1.legend();\n",
    "fig1.set_ylabel('Power[W]')\n",
    "fig1.set_xlabel('Time')\n",
    "fig1.set_title('Train data prediction');\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig2 = plt.subplot()\n",
    "x2 = range(len(predictions_test))\n",
    "fig2.plot(x2, gen.Y_test,  label='true',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig2.plot(x2, predictions_test,  label='pred',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig2.legend();\n",
    "fig2.set_ylabel('Power[W]')\n",
    "fig2.set_xlabel('Time')\n",
    "fig2.set_title('Test data prediction');\n",
    "\n",
    "fig1.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using train and test data and plot the result (NEW DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gen.model.predict(gen.X_train)\n",
    "predictions_test = gen.model.predict(gen.X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig1 = plt.subplot()\n",
    "x1 = range(len(predictions_train))\n",
    "fig1.plot(x1[100:1000], gen.Y_train[100:1000],  label='true',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig1.plot(x1[100:1000], predictions_train[100:1000],  label='pred',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig1.legend();\n",
    "fig1.set_ylabel('Power[W]')\n",
    "fig1.set_xlabel('Time')\n",
    "fig1.set_title('Train data prediction');\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig2 = plt.subplot()\n",
    "x2 = range(len(predictions_test))\n",
    "fig2.plot(x2, gen.Y_test,  label='true',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig2.plot(x2, predictions_test,  label='pred',  marker='o',linewidth=0.3, markersize=2)\n",
    "fig2.legend();\n",
    "fig2.set_ylabel('Power[W]')\n",
    "fig2.set_xlabel('Time')\n",
    "fig2.set_title('Test data prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out different sequence lengths\n",
    "seq_lengths =  [10,25,50,75,100,125,150,175,200]\n",
    "multiple = False\n",
    "n_test_ratio = 0\n",
    "#joint_angle_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_auginput.pickle', 'rb') as file:\n",
    "    joint_angle_data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "#power_data.pickle\n",
    "with open(resource_path + '/100_percent_path4_high_poweroutput.pickle', 'rb') as file:\n",
    "    power_data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "for n in range(len(seq_lengths)):\n",
    "    print('Testing seqLength: ' + str(seq_lengths[n]))\n",
    "    gen_final = Generator()\n",
    "    print('Setting up data')\n",
    "    gen_final.set_up_data(seq_lengths[n], joint_angle_data, power_data, n_test_ratio, multiple=False)\n",
    "    gen_final.set_model_to_final_model(multiple=multiple)\n",
    "    print(gen_final.X_train.shape)\n",
    "    print(gen_final.Y_train.shape)\n",
    "    name = 'SeqLength_test_Final_Model_' + str(seq_lengths[n])\n",
    "    print('Begining to train')\n",
    "    gen_final.train_model(name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For singel output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(2*len(joint_angle_data_path_list)))\n",
    "gen = Generator()\n",
    "seqLleng = 50 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = False\n",
    "print('Begining to load')\n",
    "gen.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, multiple=multiple)\n",
    "print(gen.X_train.shape)\n",
    "print(gen.Y_train.shape)\n",
    "\n",
    "print('Saving the data')\n",
    "#joint_angle_data.pickle\n",
    "with open('data/temp/joint_angle_data_training_data.pickle', 'wb') as file:\n",
    "    pickle.dump(gen.X_train, file)\n",
    "    file.close()\n",
    "\n",
    "#power_data.pickle\n",
    "with open('data/temp/power_data_training_data.pickle', 'wb') as file:\n",
    "    pickle.dump(gen.Y_train, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seqLleng = 100 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = False\n",
    "print('Begining to load')\n",
    "gen_final.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, multiple=multiple)\n",
    "print(gen_final.X_train.shape)\n",
    "print(gen_final.Y_train.shape)\n",
    "\n",
    "## Setup for final training\n",
    "#gen_final = Generator()\n",
    "#seqLleng = 50 \n",
    "#multiple = False\n",
    "## Load pre pre processed data\n",
    "#print('Loading Pre Processed Data')\n",
    "#gen_final.load_pre_processed_data('data/temp/joint_angle_data_training_data.pickle','data/temp/power_data_training_data.pickle')\n",
    "#print(gen_final.X_train.shape)\n",
    "#print(gen_final.Y_train.shape)\n",
    "print('Initializing model')\n",
    "gen_final.set_model_to_final_model(multiple=multiple)\n",
    "\n",
    "# The training\n",
    "n_iterations = 3\n",
    "for n in range(n_iterations):\n",
    "    print('Iteration: ' + str(n+1))\n",
    "    name = 'Final_Model_Iteration_' + str(n+1)\n",
    "    gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For hard drives with less RAM\n",
    "\n",
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seq_length = 100 \n",
    "multiple = False\n",
    "\n",
    "# The training\n",
    "n_iterations = 3\n",
    "n_test_ratio = 0\n",
    "first_run = True\n",
    "for n in range(n_iterations):\n",
    "    num_loaded_loop = 0\n",
    "    print('Iteration: ' + str(n+1))\n",
    "    for m in range(len(joint_angle_data_path_list)):\n",
    "        print('Begining to load')\n",
    "        print('Work: ' + str(m+1) + ' of ' + str(len(joint_angle_data_path_list)))\n",
    "        #joint_angle_data.pickle\n",
    "        with open(joint_angle_data_path_list[m], 'rb') as file:\n",
    "            joint_angle_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        #power_data.pickle\n",
    "        with open(power_data_path_list[match_list_vector[m]], 'rb') as file:\n",
    "            power_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        if(int(joint_angle_data.shape[0]) != int(power_data.shape[0])):\n",
    "            print('Files not equal in sizes! could not use.')\n",
    "            continue\n",
    "        \n",
    "        gen_final.set_up_data(seq_length, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "        print(gen_final.X_train.shape)\n",
    "        print(gen_final.Y_train.shape)\n",
    "        if (first_run):\n",
    "            first_run = False\n",
    "            print('Initializing model')\n",
    "            gen_final.set_model_to_final_model(multiple=multiple)\n",
    "        \n",
    "        name = 'Final_Model_singleOut_Iteration_' + str(n+1) + '_trainingNum_' + str(m+1)\n",
    "        gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For hard drives with less RAM\n",
    "\n",
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seq_length = 100 \n",
    "multiple = True\n",
    "\n",
    "# The training\n",
    "n_iterations = 2\n",
    "n_test_ratio = 0\n",
    "first_run = True\n",
    "for n in range(n_iterations):\n",
    "    num_loaded_loop = 0\n",
    "    print('Iteration: ' + str(n+1))\n",
    "    for m in range(len(joint_angle_data_path_list)):\n",
    "        print('Begining to load')\n",
    "        print('Work: ' + str(m+1) + ' of ' + str(len(joint_angle_data_path_list)))\n",
    "        #joint_angle_data.pickle\n",
    "        with open(joint_angle_data_path_list[m], 'rb') as file:\n",
    "            joint_angle_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        #power_data.pickle\n",
    "        with open(power_data_path_list[match_list_vector[m]], 'rb') as file:\n",
    "            power_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        if(int(joint_angle_data.shape[0]) != int(power_data.shape[0])):\n",
    "            print('Files not equal in sizes! could not use.')\n",
    "            continue\n",
    "        \n",
    "        gen_final.set_up_data(seq_length, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "        print(gen_final.X_train.shape)\n",
    "        print(gen_final.Y_train.shape)\n",
    "        if (first_run):\n",
    "            first_run = False\n",
    "            print('Initializing model')\n",
    "            gen_final.set_model_to_final_model(multiple=multiple)\n",
    "        \n",
    "        name = 'Final_Model_multipeOut_Iteration_' + str(n+1) + '_trainingNum_' + str(m+1)\n",
    "        gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test, do not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For hard drives with less RAM\n",
    "\n",
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seq_length = 100 \n",
    "multiple = False\n",
    "\n",
    "# The training\n",
    "n_iterations = 2\n",
    "n_test_ratio = 0\n",
    "first_run = True\n",
    "for n in range(n_iterations):\n",
    "    num_loaded_loop = 0\n",
    "    print('Iteration: ' + str(n+1))\n",
    "    for m in range(len(joint_angle_data_path_list)):\n",
    "        print('Begining to load')\n",
    "        print('Work: ' + str(m+1) + ' of ' + str(len(joint_angle_data_path_list)))\n",
    "        #joint_angle_data.pickle\n",
    "        with open(joint_angle_data_path_list[m], 'rb') as file:\n",
    "            joint_angle_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        #power_data.pickle\n",
    "        with open(power_data_path_list[match_list_vector[m]], 'rb') as file:\n",
    "            power_data = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "        if(int(joint_angle_data.shape[0]) != int(power_data.shape[0])):\n",
    "            print('Files not equal in sizes! could not use.')\n",
    "            continue\n",
    "        \n",
    "        gen_final.set_up_data(seq_length, joint_angle_data, power_data, n_test_ratio, multiple=multiple)\n",
    "        print(gen_final.X_train.shape)\n",
    "        print(gen_final.Y_train.shape)\n",
    "        if (first_run):\n",
    "            first_run = False\n",
    "            print('Initializing model')\n",
    "            gen_final.set_model_to_FFNN(multiple=multiple)\n",
    "        \n",
    "        name = 'Final_Model_FNN_Iteration_' + str(n+1) + '_trainingNum_' + str(m+1)\n",
    "        gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seqLleng = 100 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = True\n",
    "print('Begining to load')\n",
    "gen_final.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, multiple=multiple)\n",
    "print(gen_final.X_train.shape)\n",
    "print(gen_final.Y_train.shape)\n",
    "\n",
    "## Setup for final training\n",
    "#gen_final = Generator()\n",
    "#seqLleng = 50 \n",
    "#multiple = False\n",
    "## Load pre pre processed data\n",
    "#print('Loading Pre Processed Data')\n",
    "#gen_final.load_pre_processed_data('data/temp/joint_angle_data_training_data.pickle','data/temp/power_data_training_data.pickle')\n",
    "#print(gen_final.X_train.shape)\n",
    "#print(gen_final.Y_train.shape)\n",
    "print('Initializing model')\n",
    "gen_final.set_model_to_final_model(multiple=multiple)\n",
    "\n",
    "# The training\n",
    "n_iterations = 3\n",
    "for n in range(n_iterations):\n",
    "    print('Iteration: ' + str(n+1))\n",
    "    name = 'Final_Model_Iteration_multiple_' + str(n+1)\n",
    "    gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESSA ÄR DE TRE MODELERNA SOM SKA KÖRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seqLleng = 100 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = False\n",
    "print('Begining to load')\n",
    "gen_final.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, limited_data=True, multiple=multiple)\n",
    "print(gen_final.X_train.shape)\n",
    "print(gen_final.Y_train.shape)\n",
    "\n",
    "print('Initializing model')\n",
    "gen_final.set_model_to_final_model(multiple=multiple)\n",
    "\n",
    "# The training\n",
    "name = 'SUPER_limited_Final_Model_singleOutput'\n",
    "gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seqLleng = 100 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = True\n",
    "print('Begining to load')\n",
    "gen_final.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, limited_data=True, multiple=multiple)\n",
    "print(gen_final.X_train.shape)\n",
    "print(gen_final.Y_train.shape)\n",
    "\n",
    "print('Initializing model')\n",
    "gen_final.set_model_to_final_model(multiple=multiple)\n",
    "\n",
    "# The training\n",
    "name = 'SUPER_limited_Final_Model_multipleOutput'\n",
    "gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training data and save as an pickle\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "for name in os.listdir(resource_path):\n",
    "    path = os.path.join(resource_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "print('Number of paths to load: ' + str(len(joint_angle_data_path_list)))\n",
    "gen_final = Generator()\n",
    "seqLleng = 100 \n",
    "n_test = 0\n",
    "num_loaded = len(joint_angle_data_path_list)+1\n",
    "multiple = False\n",
    "print('Begining to load')\n",
    "gen_final.set_up_multi_data(seqLleng, joint_angle_data_path_list, power_data_path_list, match_list_vector, num_loaded, limited_data=True, multiple=multiple)\n",
    "print(gen_final.X_train.shape)\n",
    "print(gen_final.Y_train.shape)\n",
    "\n",
    "print('Initializing model')\n",
    "gen_final.set_model_to_FFNN()\n",
    "\n",
    "# The training\n",
    "name = 'SUPER_limited_Final_Model_FNN'\n",
    "gen_final.train_model(name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set prediction using the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLleng = 100 #12ms/leng unit\n",
    "multiple = False\n",
    "\n",
    "test_set_path = 'Pickled_data/test_sets'\n",
    "\n",
    "joint_angle_data_path_list = []\n",
    "power_data_path_list = []\n",
    "\n",
    "# Load all the test sets\n",
    "for name in os.listdir(test_set_path):\n",
    "    path = os.path.join(test_set_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "fig = plt.subplot(nrows=3, ncols=3)        \n",
    "        \n",
    "for n in range(9):\n",
    "    fig.subplot(n)\n",
    "    print('Working with path number: ' + str(n+1) + ' of 9')\n",
    "    gen_final = Generator()\n",
    "    gen_final.load_pre_processed_data(joint_angle_data_path_list[n],power_data_path_list[match_list_vector[n]])\n",
    "    num_loaded = 2\n",
    "    gen_final.set_up_multi_data(seqLleng, [joint_angle_data_path_list[n]], [power_data_path_list[match_list_vector[n]]], [0], num_loaded, multiple=multiple)\n",
    "    gen_final.set_model_to_final_model(multiple=multiple)\n",
    "    gen_final.pre_train()\n",
    "    gen_final.model.load_weights('data/weights/model_Final_Model_singleOut_Iteration_2_trainingNum_42.hdf5')\n",
    "    print('Prediction ')\n",
    "    predictions_test = gen_final.model.predict(gen_final.X_train)\n",
    "    # Figure plotting\n",
    "    x = range(len(predictions_test))\n",
    "    fig.plot(x[100:1000], gen_final.Y_train[100:1000],  label='true',  marker='o',linewidth=0.3, markersize=4)\n",
    "    fig.plot(x[100:1000], predictions_test[100:1000],  label='pred',  marker='o',linewidth=0.3, markersize=4)\n",
    "    fig.legend();\n",
    "    fig.set_ylabel('Power [W]')\n",
    "    fig.set_xlabel('Time [s]')\n",
    "    fig.set_title('Test data prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Figures  (Just some test examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_test = gen_final.model.predict(gen_final.X_train)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "fig = plt.subplot(3,3)\n",
    "\n",
    "subplot(1)\n",
    "x = range(len(predictions_test))\n",
    "fig.plot(x[100:1000], gen_final.Y_train[100:1000],  label='true',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig.plot(x[100:1000], predictions_test[100:1000],  label='pred',  marker='o',linewidth=0.3, markersize=4)\n",
    "fig.legend();\n",
    "fig.set_ylabel('Power [W]')\n",
    "fig.set_xlabel('Time [s]')\n",
    "fig.set_title('Test data prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_paths = []\n",
    "# Load all the test sets\n",
    "for name in os.listdir(test_set_path):\n",
    "    path = os.path.join(test_set_path, name)\n",
    "    if \"_auginput.pickle\" in name:\n",
    "        joint_angle_data_path_list.append(path)\n",
    "    if \"_poweroutput.pickle\" in name:\n",
    "        power_data_path_list.append(path)\n",
    "\n",
    "match_list_vector = []\n",
    "for filename in joint_angle_data_path_list:\n",
    "    name = filename[:-len(\"_auginput.pickle\")]\n",
    "    num = 0\n",
    "    for filename in power_data_path_list:\n",
    "        if name in filename:\n",
    "            match_list_vector.append(num)\n",
    "            continue\n",
    "        num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
